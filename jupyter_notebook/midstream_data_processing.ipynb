{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "## Extract PRELIM results for OCI, PRELIM and Haverly assays, including product slates, emission, API gravity, and sulfur content\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join    \n",
    "sp_dir = '/Users/rwang/RMI/Climate Action Engine - Documents/OCI Phase 2'\n",
    "\n",
    "\n",
    "print('Extracting product slates and emission data from Liam batch run results...')\n",
    "onehundredyr_path= [sp_dir + '/Midstream/Liam_Batchrun/OCI 3.0 (100-y GWP)/Haverly 100y',\n",
    "                    sp_dir + '/Midstream/Liam_Batchrun/OCI 3.0 (100-y GWP)/OCI 100y',\n",
    "                    sp_dir + '/Midstream/Liam_Batchrun/OCI 3.0 (100-y GWP)/PRELIM 100y']\n",
    "\n",
    "\n",
    "twentyyr_path = [sp_dir + '/Midstream/Liam_Batchrun/OCI 3.0 (20-y GWP)/New Results/Haverly 20y',\n",
    "                sp_dir +  '/Midstream/Liam_Batchrun/OCI 3.0 (20-y GWP)/New Results/OCI 20y', \n",
    "                sp_dir +  '/Midstream/Liam_Batchrun/OCI 3.0 (20-y GWP)/New Results/PRELIM 20y']\n",
    "\n",
    "\n",
    "# Remove files that are wrongly in the 100yr Haverly folder, based on the file name in new 20 yr haverly folder\n",
    "# This is a one time fix and should only be run once \n",
    "# for filename in os.listdir(onehundredyr_path[0]):\n",
    "#     if filename not in os.listdir(sp_dir + '/Midstream/Liam_Batchrun/OCI 3.0 (20-y GWP)/New Results/Haverly 20y'):\n",
    "#         print(filename)\n",
    "#         os.remove(onehundredyr_path[0]+'/'+filename)\n",
    "\n",
    "def extract_product_slate(file):\n",
    "    \"\"\"A function that extracts product slate mass flows from the a batch run file.\n",
    "    The input of is the file direcotyr and the output is a column of product slate\"\"\"\n",
    "    df = pd.read_excel(file,sheet_name=0, header = None)\n",
    "    assay_id = file.split('/')[-1].split('_')[0]\n",
    "    assay_name = df.iloc[0,0]\n",
    "    #print(assay_name)\n",
    "    df = df.iloc[:,1]\n",
    "    #get emission fraction data from the 3d tab (use Coking Refinery)  \n",
    "    emission = pd.read_excel(file,sheet_name=2, header = None)\n",
    "\n",
    "    df = pd.concat([df,emission.iloc[1,0:3].T],axis=0)\n",
    "    default_refinery = df.iloc[0] #'_'.join(df.iloc[0].split('_')[:-2])\n",
    "    df = df.reset_index()\n",
    "    #print(df)\n",
    "    df = df.drop(columns ='index')\n",
    "    \n",
    "    df.iloc[0,0] = assay_name\n",
    "\n",
    "    df.iloc[1,0] = default_refinery\n",
    "    df.iloc[2,0]=assay_id\n",
    "    #assay_name\n",
    "    return df\n",
    "\n",
    "def midstream_extraction(fpath):\n",
    "    '''Extrac midstream product slates and emissions data given\n",
    "    Inputs: filepath for 100yr or 20yr GWP prelim run results\n",
    "    Outputs: a dataframe of assay library'''\n",
    "\n",
    "    assay_file_list = dict()\n",
    "    for directory in fpath:\n",
    "        folder = directory.split('/')[-1].split(' ')[0].split(' ')[0].lower()\n",
    "        assay_file_list[folder] = []\n",
    "        \n",
    "        for file in os.listdir(directory):\n",
    "            filename = os.fsdecode(file)\n",
    "            if filename.endswith('.xlsx'): \n",
    "    #            print(filename)\n",
    "                assay_file_list[folder].append(join(directory, filename))\n",
    "\n",
    "\n",
    "    # The parameter names are extracted into a series \n",
    "    parameter = pd.read_excel(assay_file_list['oci'][0],sheet_name=0, header=None)\n",
    "    parameter = parameter.iloc[:,0]\n",
    "\n",
    "    parameter = pd.concat([parameter,pd.DataFrame(['emission_frac_CO2','emission_frac_CH4','emission_frac_N2O'])],axis =0)\n",
    "    parameter = parameter.reset_index().drop(columns ='index')\n",
    "\n",
    "\n",
    "\n",
    "    parameter.iloc[0,0]='parameter'\n",
    "    parameter.iloc[1,0]='Default Refinery'\n",
    "    parameter.iloc[2,0]='assay_id'\n",
    "    # store all one hundred year assay product slates in a dictionary with assay group as keys and assay product slate dataframes as values\n",
    "    assay_slates_df = dict()\n",
    "    for assay_group in assay_file_list:\n",
    "        #print(assay_group)\n",
    "        df_list = []\n",
    "        for assay_file in assay_file_list[assay_group]:\n",
    "            df_list.append(extract_product_slate(assay_file))        \n",
    "        df =pd.concat([parameter,pd.concat(df_list,axis = 1)],axis=1)\n",
    "        df.columns = df.iloc[0] \n",
    "        df = df[1:]\n",
    "        df = df.T\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df[1:]\n",
    "        df = df.astype('float',errors='ignore')\n",
    "        #df['assay_group']=assay_group\n",
    "        assay_slates_df[assay_group]= df    \n",
    "\n",
    "    assay_library = pd.concat(assay_slates_df,axis = 0)\n",
    "\n",
    "    assay_library.reset_index(inplace = True)\n",
    "\n",
    "\n",
    "    assay_library.rename(columns={'level_0':'assay_group',0:'assay_name'},inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "    #Take the first assay of duplicate assays in each assay group\n",
    "\n",
    "    #assay_library = assay_library.groupby(['assay_group','assay_name']).first()\n",
    "\n",
    "\n",
    "\n",
    "    assay_library = assay_library.dropna(axis = 1,how ='all')\n",
    "\n",
    "    assay_library.reset_index(inplace = True)\n",
    "\n",
    "    assay_library.drop_duplicates(subset='assay_name',keep='first')\n",
    "\n",
    "    assay_library['assay_name'] = assay_library['assay_name'].apply(lambda x: x.strip())\n",
    "    assay_library.drop(columns = 'index',inplace = True)\n",
    "    return assay_library\n",
    "\n",
    "onehundred_df = midstream_extraction(onehundredyr_path)\n",
    "onehundred_df['gwp'] = '100'\n",
    "twenty_df = midstream_extraction(twentyyr_path)\n",
    "twenty_df['gwp'] = '20'\n",
    "final_assay_library = pd.concat([onehundred_df, twenty_df]).reset_index().drop(columns = 'index')\n",
    "# Use the old twenty year file names to get correct assay names. Reason: 100 year file name is not clean. \n",
    "assay_name_path = {'haverly': sp_dir + '/Midstream/Liam_Batchrun/OCI 3.0 (20-y GWP)/Old Results/Haverly 20y',\n",
    "                          'oci': sp_dir + '/Midstream/Liam_Batchrun/OCI 3.0 (20-y GWP)/Old Results/OCI 20y',\n",
    "                          'prelim': sp_dir + '/Midstream/Liam_Batchrun/OCI 3.0 (20-y GWP)/Old Results/PRELIM 20y'}                              \n",
    "\n",
    "def assay_name_20yr(assay_group,assay_id):\n",
    "    '''return the right assay name based on the file names in the old 20 year direcotry.'''\n",
    "    for filename in os.listdir(assay_name_path[assay_group]):\n",
    "        if filename.split('_')[0]==assay_id:\n",
    "            assay_name = '_'.join(filename.split('_')[1:])[:-5]\n",
    "            return assay_name.strip()\n",
    "\n",
    "final_assay_library['assay_name']=final_assay_library.apply(lambda x: assay_name_20yr(x['assay_group'],x['assay_id']),axis=1)\n",
    "\n",
    "\n",
    "# Get throughput and sulfur content values from the three assay files and merge into the assay library\n",
    "\n",
    "assay_files = {'haverly':[sp_dir + '/Midstream/Liam_Batchrun/OCI 3.0 (100-y GWP)/Haverly PRELIM Assays.xlsx',535],\n",
    "              'prelim':[sp_dir + '/Midstream/Liam_Batchrun/OCI 3.0 (100-y GWP)/PRELIM Assays.xlsx',149],\n",
    "              'oci':[sp_dir + '/Midstream/Liam_Batchrun/OCI 3.0 (100-y GWP)/OCI Assay List Expanded 107.xlsx',107]}\n",
    "\n",
    "assay_bbl_sulfur=dict()\n",
    "for assay in assay_files:\n",
    "    assay_df=pd.read_excel(assay_files[assay][0],header=None)\n",
    "    assay_name = []\n",
    "    assay_throughput =[]\n",
    "    assay_sulfur = []\n",
    "    assay_gravity = []\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(assay_files[assay][1]):\n",
    "        assay_name.append(assay_df.iloc[i*15,0].strip())\n",
    "        assay_throughput.append(float(assay_df.iloc[3+i*15,2]))\n",
    "        assay_sulfur.append(float(assay_df.iloc[6+i*15,2]))\n",
    "        assay_gravity.append(float(assay_df.iloc[8+i*15,2]))\n",
    "    df['assay_name']=assay_name\n",
    "    df['throughput']=assay_throughput\n",
    "    df['sulfur']=assay_sulfur\n",
    "    df['gravity']=assay_gravity\n",
    "    assay_bbl_sulfur[assay]=df\n",
    "    \n",
    "\n",
    "assay_bbl_sulfur_library = pd.concat(assay_bbl_sulfur,axis = 0)\n",
    "\n",
    "assay_bbl_sulfur_library.reset_index(level = 0, inplace = True)\n",
    "\n",
    "assay_bbl_sulfur_library.rename(columns ={'level_0':'assay_group'},inplace = True)\n",
    "\n",
    "assay_bbl_sulfur_library.reset_index(inplace = True)\n",
    "\n",
    "assay_bbl_sulfur_library.drop(columns = 'index',inplace = True)\n",
    "\n",
    "#Take the first assay of duplicate assays in each assay group\n",
    "assay_bbl_sulfur_library = assay_bbl_sulfur_library.groupby(['assay_group','assay_name']).first()\n",
    "\n",
    "assay_bbl_sulfur_library.reset_index(inplace = True)\n",
    "\n",
    "\n",
    "#assay_bbl_sulfur_library.to_excel(sp_dir + '/Midstream/Liam_Batchrun/Analytics/assay_bbl_sulfur_library.xlsx',index = False)\n",
    "\n",
    "final_assay_library_merged = final_assay_library.merge(assay_bbl_sulfur_library,how = 'left',indicator = True)\n",
    "\n",
    "\n",
    "if final_assay_library_merged[final_assay_library_merged['_merge']!='both'].shape[0]>0:\n",
    "    print('unmerged, check results.')\n",
    "    print(final_assay_library_merged[final_assay_library_merged['_merge']!='both'])\n",
    "else:\n",
    "    final_assay_library_merged.drop(columns = '_merge',inplace = True)\n",
    "\n",
    "\n",
    "final_assay_library_merged.to_excel(sp_dir + '/Midstream/Liam_Batchrun/Analytics/final_assay_library.xlsx',index = False)\n",
    "\n",
    "# ## Mapping OPGEE modelled fields to OCI, PRELIM and Haverly Assays based on API gravity and Surfur content\n",
    "pub_data = pd.read_excel(sp_dir + '/Upstream/Public Data Batch runs/Scraped Public Data.xlsx')\n",
    "field_assay = pub_data[['Field location (Country)', 'Field name', 'Assay Name']].dropna()\n",
    "field_assay['Field name'] = field_assay['Field name'].apply(lambda x: x.strip())\n",
    "field_assay['Field location (Country)'] = field_assay['Field location (Country)'].apply(lambda x: x.strip())\n",
    "\n",
    "midstream = field_assay.merge(final_assay_library_merged,left_on = 'Assay Name', right_on = 'assay_name',how = 'left', indicator = True)\n",
    "\n",
    "# only select one assay if there are multiple assays with the same name matched to the field\n",
    "midstream = midstream.groupby(['Field name','gwp']).first()\n",
    "\n",
    "if midstream[midstream['_merge']!='both'].shape[0]==0:\n",
    "    midstream.drop(columns = '_merge',inplace = True)\n",
    "else:\n",
    "    print('not fully merged. please check')\n",
    "    print(midstream[midstream['_merge']!='both'])\n",
    "midstream.reset_index(inplace = True)\n",
    "midstream.to_csv(sp_dir+'/Upstream/upstream_data_pipeline_sp/Postprocessed_Outputs_2/midstream_postprocessed.csv')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting product slates and emission data from Liam batch run results...\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "5ccea0d9b7a70875400646c164148aa592b31bf40541a4c6f7aff7a306f1f8ff"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}